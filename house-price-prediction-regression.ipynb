{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Task**: It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n\n**Evaluation**: Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price."},{"metadata":{},"cell_type":"markdown","source":"This notebook has been created after studying [this notebook.](https://www.kaggle.com/ankitverma2010/house-prices-prediction-beginner-to-advanced#Exploratory-Data-Analysis)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Python imports\nimport os\n\n# Maths and data imports\nimport numpy as np\nimport pandas as pd\n\n# Plot imports\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# ML modeling imports\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error as mse\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv', index_col='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train.copy()\ntest_df = test.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MSSubClass, OverallQual and OverallCond and categorical variable, lets convert it into one.\ntrain_df['MSSubClass'] = train_df['MSSubClass'].astype('object')\ntrain_df['OverallQual'] = train_df['OverallQual'].astype('object')\ntrain_df['OverallCond'] = train_df['OverallCond'].astype('object')\n\ntest_df['MSSubClass'] = test_df['MSSubClass'].astype('object')\ntest_df['OverallQual'] = test_df['OverallQual'].astype('object')\ntest_df['OverallCond'] = test_df['OverallCond'].astype('object')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now seperate the categorical and numerical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_var_dtype_list(df):\n    cat_cols = []\n    num_cols = []\n\n    for col in df.columns:\n        if df[col].dtypes == 'object':\n            cat_cols.append(col)\n        else:\n            num_cols.append(col)\n    return (cat_cols, num_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols_train, num_cols_train = get_var_dtype_list(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols_test, num_cols_test = get_var_dtype_list(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take log of SalePrice as our evaluation metric is log RMS value."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['SalePrice'] = np.log(train_df['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA\n\nIn EDA we will analyse data to see if there is any - \n1. Skewness in features\n2. Check for missing values and ouliers and fix them.\n3. Check the variability of different features and scale them.\n4. Check for multicolinearity among multiple exploratory variables (features).\n5. Check if response variable is correlated to any/may exploratory variable(s).\n6. Analyse the target/response variable."},{"metadata":{},"cell_type":"markdown","source":"### Lets check distribution fo continuous columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(len(num_cols_train)//5, 5, figsize=(30, 30))\nfor col, ax in zip(num_cols_train[:-1], axs.flatten()):\n    sns.distplot(train_df[col], ax=ax)\n    ax.set_title(col)\n    ax.set_xlabel('')\n    ax.set_ylabel('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n\n1. There is  large variation in the scale of each continuous variables, hence, we need to scale them.\n2. Features such as YearBuilt and GarageYrBlt are left skewed, indicating that more houses were build in the later years hence, more garages were also built then. So, we might want to leave such variable for outlier check.\n3. Exploratory varibales - 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'LowQualFinSF', 'BsmtHalfBath', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal' are heavily right skewed.\n\nLet's further explore these variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"right_skewed = ['LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n                'LowQualFinSF', 'BsmtHalfBath', 'WoodDeckSF', 'OpenPorchSF', \n                'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_skew_desc = train_df[right_skewed].describe().T\nr_skew_desc['coef_of_var'] = r_skew_desc['std']/r_skew_desc['mean']\n\nr_skew_desc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see a huge variation in lot of features. For now we can drop the columns with coefficient of variation > 3.\n\nAs the number of missing values is zero or near to zero in most of the features we would like to further analyse them (maybe take log to reduce the right skewness) and embrace the variability.\n\nBut for now lets got with dropping them off, and compare the effects in the next iteration/followup."},{"metadata":{"trusted":true},"cell_type":"code","source":"r_skew_desc[r_skew_desc['coef_of_var']>3].T.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_skew_cols = ['BsmtFinSF2', 'LowQualFinSF', 'BsmtHalfBath', '3SsnPorch',\n                  'ScreenPorch', 'PoolArea', 'MiscVal']\n\ntrain_df.drop(drop_skew_cols, axis=1, inplace=True)\ntest_df.drop(drop_skew_cols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's update out list of numerical columns list."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols_train, num_cols_train = get_var_dtype_list(train_df)\ncat_cols_test, num_cols_test = get_var_dtype_list(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let check and impute missing values in continuous variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_missing_stats(df, col_list, threshold=0):\n    total = len(df)\n    for col in col_list:\n        null = df[col].isnull().sum()\n        if null > 0 and null/total >= threshold:\n            print(col)\n            if df[col].dtypes == 'object':\n                print(df[col].value_counts())\n            print(f'Missing values: {null} of {total}')\n            print(f'Percent missing values: {round((null*100)/total, 2)}%\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_missing_stats(train_df, num_cols_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_missing_stats(test_df, num_cols_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It does not seem to be extreme case of missing values so, we can simply replace by the median (as the data seem to be highly variable and median is a robust metric) of training set in both the training and test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer = SimpleImputer(strategy='median')\ntrain_df[num_cols_train[:-1]] = imputer.fit_transform(train_df[num_cols_train[:-1]])\ntest_df[num_cols_test] = imputer.transform(test_df[num_cols_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_missing_stats(train_df, num_cols_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_missing_stats(test_df, num_cols_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, the missing values for numerical columns have disappeared. Now, lets hit the categorical columns."},{"metadata":{},"cell_type":"markdown","source":"### Now, lets check and impute missing values of categorical columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"get_missing_stats(train_df, cat_cols_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_missing_stats(test_df, cat_cols_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some columns seet to have quite a number of missing values. Lets further analyse them using barplot."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(len(cat_cols_train)//5, 5, figsize=(30, 40))\nfor col, ax in zip(cat_cols_train, axs.flatten()):\n    y = train_df[col].value_counts()\n    ax.bar(y.index, y.values)\n    ax.set_title(col)\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(35)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's just analyse the columns with missing frequency greater than 40%."},{"metadata":{"trusted":true},"cell_type":"code","source":"get_missing_stats(train_df, cat_cols_train, 0.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_missing_stats(test_df, cat_cols_test, 0.4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can infer two cases from above:\n1. The houses did not have these features\n2. The houses did have these features but were not reported.\n\nHowever, if we think then it seems feasible to assume that all the houses surveyed did not have all the 80 features. So, for now we can impute the missing values (with freq. > 40%) with N.A, where as missing values of columns with freq < 40% with most frequent value.\n\nI think it will be safe to assume and hypothesise that maybe the localities in which the houses were surveyed did not have the above features, thus introducing selective bias in data collection. Although, we can not be certain of it."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(['PoolQC'], axis=1, inplace=True)\ntest_df.drop(['PoolQC'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"na_cols = ['Alley','FireplaceQu','Fence','MiscFeature']\n\nna_imputer = SimpleImputer(strategy='constant', fill_value='N.A')\ntrain_df[na_cols] = na_imputer.fit_transform(train_df[na_cols])\ntest_df[na_cols] = na_imputer.transform(test_df[na_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols_train, num_cols_train = get_var_dtype_list(train_df)\ncat_cols_test, num_cols_test = get_var_dtype_list(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mf_imputer = SimpleImputer(strategy='most_frequent')\ntrain_df[cat_cols_train] = mf_imputer.fit_transform(train_df[cat_cols_train])\ntest_df[cat_cols_test] = mf_imputer.transform(test_df[cat_cols_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_missing_stats(train_df, cat_cols_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_missing_stats(test_df, cat_cols_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we have dealt with missing values. Now, lets check for multicolinearity among exploratory variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nsns.heatmap(train_df.drop(['SalePrice'], axis=1).corr(), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor = train_df.drop(['SalePrice'], axis=1).corr()\nfor i, col in enumerate(cor.columns):\n    for row in cor.index[i+1:]:\n        if col != row and cor[col][row] > 0.7:\n            print(f'({row}, {col}): {cor[col][row]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hmm...From the overwhelming plots above we can see that few varibles are highly correlated, and it makes sense for them to be correlated. Hence, for now we might want to keep them."},{"metadata":{},"cell_type":"markdown","source":"### Scale variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\ntrain_df[num_cols_train[:-1]] = scaler.fit_transform(train_df[num_cols_train[:-1]])\ntest_df[num_cols_test] = scaler.transform(test_df[num_cols_test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One-hot encoding of categorical columns\n\nFor now, let's simply one hot encode the categorical columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols_train, num_cols_train = get_var_dtype_list(train_df)\ncat_cols_test, num_cols_test = get_var_dtype_list(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.get_dummies(train_df, drop_first=True, columns=cat_cols_train)\ntest_df = pd.get_dummies(test_df, drop_first=True, columns=cat_cols_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check train and test for compatability"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if both train and test contain same columns\ntrain_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compat_list = list(set(train_df.columns).intersection(test_df.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(compat_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, y = train_df[compat_list], train_df['SalePrice']\ntest_X = test_df[compat_list]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(train_X.columns == test_X.columns).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, valid_x, train_y, valid_y = train_test_split(train_X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_regressor = LinearRegression()\nlinear_regressor.fit(train_x, train_y)\npreds = linear_regressor.predict(valid_x)\nmse(valid_y, preds, squared=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {\n    'RFR': RandomForestRegressor,\n    'ADR': AdaBoostRegressor,\n    'XGB': XGBRegressor\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_model(name, model, train_ds, valid_ds):\n    X, y = train_ds\n    X_val, y_val = valid_ds\n    \n    model.fit(X, y)\n    y_hat = model.predict(X)\n    y_hat_val = model.predict(X_val)\n    \n    mse_ = mse(y, y_hat, squared=False)\n    mse_val = mse(y_val, y_hat_val, squared=False)\n    \n    print(f'Model: {name}, Train MSE: {mse_}, Val MSE: {mse_val}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_est = [10, 25, 50, 100, 125]\nfor i in range(len(n_est)):\n    print(f'n_estimators: {n_est[i]}')\n    for name, model in models.items():\n        model = model(n_estimators=n_est[i])\n        fit_model(name, model, (train_x, train_y), (valid_x, valid_y))\n    print('-'*20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`n_estimators=25` seems like a sweet spot for `AdaBoostRegressor` and `XGBRegressor`. Lets ensemble te ensembles"},{"metadata":{"trusted":true},"cell_type":"code","source":"abr = AdaBoostRegressor(n_estimators=25)\nxgr = XGBRegressor(n_estimators=25)\n\nabr.fit(train_x, train_y)\nxgr.fit(train_x, train_y)\n\np1 = abr.predict(valid_x)\np2 = xgr.predict(valid_x)\np3 = linear_regressor.predict(valid_x)\n\nm1 = mse(valid_y, p1, squared=False)\nm2 = mse(valid_y, p2, squared=False)\nm3 = mse(valid_y, p3, squared=False)\nm4 = mse(valid_y, (p1+p2)/2, squared=False)\nm5 = mse(valid_y, (p1+p2+p3)/3, squared=False)\n\nprint(f'Ensemble MSE: \\nABR: {m1}\\nXGR: {m2}\\nLinear: {m3}\\nABR+XGR: {m4}\\nABR+XGR+Linear: {m5}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try them all, \n* linear regressor\n* Ensemble of AdaBoost and XGBoost\n* Ensemble of all of the above"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_1 = linear_regressor.predict(test_X)\npred_2 = (abr.predict(test_X) + xgr.predict(test_X))/2\npred_3 = (2*pred_2 + pred_1)/3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_submission(preds, name='submission.csv'):\n    preds = np.expm1(preds)\n    submission = pd.DataFrame({'Id': test_X.index, 'SalePrice': preds})\n    submission.to_csv(name, index=False)\n    return submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_submission(pred_1, 'submission1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_submission(pred_2, 'submission2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_submission(pred_3, 'submission3.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## References:\n\nhttps://www.kaggle.com/ankitverma2010/house-prices-prediction-beginner-to-advanced#Exploratory-Data-Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}